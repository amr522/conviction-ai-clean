import pandas as pd
import json
from datetime import datetime
import os

def generate_performance_report():
    """Generate comprehensive performance report for HPO job hpo-full-1751555388"""
    
    print("ğŸ“Š GENERATING HPO PERFORMANCE REPORT")
    print("=" * 60)
    
    if not os.path.exists('sagemaker_hpo_results.csv'):
        print("âŒ HPO results file not found. Run analyze_sagemaker_hpo.py first.")
        return
    
    results_df = pd.read_csv('sagemaker_hpo_results.csv')
    
    total_jobs = len(results_df)
    completed_jobs = len(results_df[results_df['status'] == 'Completed'])
    
    print(f"ğŸ¯ HPO JOB SUMMARY:")
    print(f"Job Name: hpo-full-1751555388")
    print(f"Total Training Jobs: {total_jobs}")
    print(f"Completed Jobs: {completed_jobs}")
    print(f"Success Rate: {completed_jobs/total_jobs*100:.1f}%")
    
    scores = results_df['objective_value'].dropna()
    
    if len(scores) > 0:
        print(f"\nğŸ“ˆ PERFORMANCE METRICS:")
        print(f"Best AUC Score: {scores.max():.4f}")
        print(f"Worst AUC Score: {scores.min():.4f}")
        print(f"Average AUC Score: {scores.mean():.4f}")
        print(f"Median AUC Score: {scores.median():.4f}")
        print(f"Standard Deviation: {scores.std():.4f}")
        
        thresholds = [0.55, 0.60, 0.70, 0.80, 0.90, 0.95]
        print(f"\nğŸ¯ THRESHOLD ANALYSIS:")
        for threshold in thresholds:
            count = len(scores[scores >= threshold])
            percentage = count / len(scores) * 100
            print(f"AUC â‰¥ {threshold:.2f}: {count}/{len(scores)} ({percentage:.1f}%)")
        
        top_10 = results_df.nlargest(10, 'objective_value')
        print(f"\nğŸ† TOP 10 PERFORMING MODELS:")
        print("-" * 80)
        print(f"{'Rank':<4} {'Job Name':<45} {'AUC Score':<10} {'Status':<12}")
        print("-" * 80)
        
        for i, (_, row) in enumerate(top_10.iterrows()):
            print(f"{i+1:<4} {row['job_name']:<45} {row['objective_value']:<10.4f} {row['status']:<12}")
    
    report = {
        'job_name': 'hpo-full-1751555388',
        'analysis_date': datetime.now().isoformat(),
        'summary': {
            'total_jobs': int(total_jobs),
            'completed_jobs': int(completed_jobs),
            'success_rate': float(completed_jobs/total_jobs*100)
        },
        'performance': {
            'best_auc': float(scores.max()) if len(scores) > 0 else None,
            'worst_auc': float(scores.min()) if len(scores) > 0 else None,
            'average_auc': float(scores.mean()) if len(scores) > 0 else None,
            'median_auc': float(scores.median()) if len(scores) > 0 else None,
            'std_auc': float(scores.std()) if len(scores) > 0 else None
        },
        'threshold_analysis': {},
        'top_models': []
    }
    
    if len(scores) > 0:
        for threshold in thresholds:
            count = len(scores[scores >= threshold])
            percentage = count / len(scores) * 100
            report['threshold_analysis'][f'auc_ge_{threshold:.2f}'] = {
                'count': int(count),
                'total': int(len(scores)),
                'percentage': float(percentage)
            }
        
        for _, row in top_10.iterrows():
            report['top_models'].append({
                'job_name': row['job_name'],
                'auc_score': float(row['objective_value']),
                'status': row['status'],
                'creation_time': row['creation_time']
            })
    
    with open('hpo_performance_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nğŸ’¾ REPORT SAVED:")
    print(f"- Detailed JSON: hpo_performance_report.json")
    print(f"- Raw CSV: sagemaker_hpo_results.csv")
    
    print(f"\nğŸ” KEY FINDINGS:")
    if len(scores) > 0:
        if scores.max() >= 0.95:
            print("âœ… Exceptional performance achieved (AUC â‰¥ 0.95)")
        elif scores.max() >= 0.80:
            print("âœ… Strong performance achieved (AUC â‰¥ 0.80)")
        elif scores.max() >= 0.60:
            print("âœ… Good performance achieved (AUC â‰¥ 0.60)")
        else:
            print("âš ï¸ Performance below expectations")
        
        if scores.mean() >= 0.90:
            print("âœ… Consistently high performance across all models")
        elif scores.mean() >= 0.70:
            print("âœ… Generally good performance across models")
        else:
            print("âš ï¸ Variable performance across models")
    
    # Next steps
    print(f"\nğŸš€ RECOMMENDED NEXT STEPS:")
    print("1. âœ… HPO completed successfully with excellent results")
    print("2. ğŸ“¦ Locate and download best performing models from S3")
    print("3. ğŸ”„ Create ensemble from top 5-10 models")
    print("4. ğŸ“Š Generate predictions on test set")
    print("5. ğŸ¯ Prepare for production deployment")
    
    return report

if __name__ == "__main__":
    generate_performance_report()
