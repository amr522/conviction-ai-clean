name: SageMaker HPO Pipeline

on:
  schedule:
    # Run every Monday at midnight UTC
    - cron: '0 0 * * 1'
  workflow_dispatch:  # Allow manual triggering

env:
  AWS_DEFAULT_REGION: us-east-1
  S3_BUCKET: hpo-bucket-773934887314
  PYTHON_VERSION: '3.10'
  TARGET_COMPLETED: 138

jobs:
  hpo-job:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
          
      - name: Verify AWS connection
        run: |
          aws sts get-caller-identity
          aws s3 ls s3://${{ env.S3_BUCKET }} --summarize
          
      - name: Get pinned dataset
        run: |
          echo "üîç Getting pinned dataset from previous HPO job..."
          source scripts/get_last_hpo_dataset.sh
          
      - name: Echo dataset path
        run: ./ci_echo_dataset.sh            # masks + prints $PINNED_DATA_S3
          
      - name: Launch HPO job
        run: |
          echo "üöÄ Launching SageMaker HPO job with pinned dataset..."
          python aws_hpo_launch.py
          
      - name: Monitor HPO progress
        run: |
          echo "üìä Setting up HPO monitoring..."
          nohup python automated_hpo_monitor.py --job-prefix "46-models" --wait-time 600 > hpo_monitor.log &
          echo "‚úÖ HPO monitoring started in background"
          
      - name: Create status report
        run: |
          echo "üìù Generating initial HPO status report..."
          python analyze_hpo_results.py --partial --output-file hpo_initial_report.md
          
      - name: Commit status report
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Add HPO report
          git add hpo_initial_report.md
          git add last_dataset_uri.txt
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "HPO Job Started - $(date +%Y-%m-%d)
            
            - Launched new HPO job with pinned dataset
            - Generated initial status report
            - Updated dataset pinning files
            
            Automated by GitHub Actions"
            
            git push
          fi
          
      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå HPO job launch failed!"
          echo "Check the GitHub Actions logs for details."
          # In production, you could send notifications to Slack, email, etc.
