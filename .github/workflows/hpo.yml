name: HPO Pipeline with Pinned Dataset

on:
  workflow_dispatch:
    inputs:
      target_completed:
        description: 'Minimum completed training jobs for dataset selection'
        required: false
        default: '138'
        type: string
      hpo_type:
        description: 'HPO job type to run'
        required: true
        default: 'aapl'
        type: choice
        options:
        - aapl
        - full_universe
        - both

env:
  AWS_DEFAULT_REGION: us-east-1

jobs:
  pin-dataset:
    name: Pin HPO Dataset
    runs-on: ubuntu-latest
    outputs:
      pinned_data_s3: ${{ steps.pin.outputs.pinned_data_s3 }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      
      - name: Pin HPO dataset
        id: pin
        env:
          TARGET_COMPLETED: ${{ github.event.inputs.target_completed }}
        run: |
          echo "üîç Searching for HPO dataset with ‚â•$TARGET_COMPLETED completed jobs..."
          
          # Make script executable
          chmod +x scripts/get_last_hpo_dataset.sh
          
          # Run dataset pinning script
          if ./scripts/get_last_hpo_dataset.sh; then
            PINNED_DATA_S3=$(cat last_dataset_uri.txt)
            echo "pinned_data_s3=$PINNED_DATA_S3" >> $GITHUB_OUTPUT
            
            # Mask the S3 URI for security but echo for debugging
            echo "::add-mask::$PINNED_DATA_S3"
            echo "üîí Pinned dataset: $PINNED_DATA_S3"
          else
            echo "‚ùå Failed to pin dataset"
            exit 1
          fi

  run-hpo:
    name: Run HPO Job
    runs-on: ubuntu-latest
    needs: pin-dataset
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      
      - name: Run HPO with pinned dataset
        env:
          PINNED_DATA_S3: ${{ needs.pin-dataset.outputs.pinned_data_s3 }}
          HPO_TYPE: ${{ github.event.inputs.hpo_type }}
        run: |
          echo "üöÄ Running HPO with pinned dataset"
          echo "Dataset: $PINNED_DATA_S3"
          echo "HPO Type: $HPO_TYPE"
          
          # Export environment variable for Python script
          export PINNED_DATA_S3="$PINNED_DATA_S3"
          
          # Run appropriate HPO job based on input
          case "$HPO_TYPE" in
            "aapl")
              echo "Running AAPL HPO job..."
              python - << 'PYCODE'
from aws_hpo_launch import launch_aapl_hpo
import sys
job_name = launch_aapl_hpo()
if not job_name:
    sys.exit(1)
print(f'‚úÖ AAPL HPO job launched: {job_name}')
PYCODE
              ;;
            "full_universe")
              echo "Running Full Universe HPO job..."
              python - << 'PYCODE'
from aws_hpo_launch import launch_full_universe_hpo
import sys
job_name = launch_full_universe_hpo()
if not job_name:
    sys.exit(1)
print(f'‚úÖ Full Universe HPO job launched: {job_name}')
PYCODE
              ;;
            "both")
              echo "Running both AAPL and Full Universe HPO jobs..."
              python aws_hpo_launch.py
              ;;
            *)
              echo "‚ùå Invalid HPO type: $HPO_TYPE"
              exit 1
              ;;
          esac

  validate-results:
    name: Validate HPO Results
    runs-on: ubuntu-latest
    needs: [pin-dataset, run-hpo]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      
      - name: Validate HPO job status
        env:
          PINNED_DATA_S3: ${{ needs.pin-dataset.outputs.pinned_data_s3 }}
        run: |
          echo "üîç Validating HPO job results..."
          echo "Used dataset: $PINNED_DATA_S3"
          
          # List recent HPO jobs
          echo "Recent HPO jobs:"
          aws sagemaker list-hyper-parameter-tuning-jobs \
            --creation-time-after "$(date -d '1 hour ago' -u +%Y-%m-%dT%H:%M:%SZ)" \
            --query 'HyperParameterTuningJobSummaries[*].[HyperParameterTuningJobName,HyperParameterTuningJobStatus,CreationTime]' \
            --output table
          
          echo "‚úÖ HPO pipeline validation complete"

  monitor:
    needs: run-hpo
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      
      - name: Wait & verify HPO success
        run: |
          for i in {1..12}; do
            status=$(aws sagemaker describe-hyper-parameter-tuning-job \
              --hyper-parameter-tuning-job-name ${{ github.run_id }} \
              --query 'HyperParameterTuningJobStatus' --output text)
            if [[ "$status" == "Completed" ]]; then
              echo "‚úÖ HPO Completed"
              exit 0
            fi
            sleep 60
          done
          echo "‚ùå HPO did not complete within expected time" && exit 1
